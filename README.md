# Skema

> **Intelligent Requirement Classification System**

![Status](https://img.shields.io/badge/Status-Beta-blue) ![Architecture](https://img.shields.io/badge/Architecture-Event--Driven-orange) ![Python](https://img.shields.io/badge/Python-3.11-yellow)

## ðŸ”­ VisiÃ³n del Proyecto

Skema es un sistema de ingenierÃ­a de datos diseÃ±ado para **automatizar la clasificaciÃ³n de requerimientos entrantes** en equipos de ingenierÃ­a de alto volumen.

El problema que resuelve es la fricciÃ³n cognitiva que genera clasificar manualmente miles de tickets. Skema ingesta flujos de informaciÃ³n no estructurada (Jira, Slack, Email), los procesa mediante ML y entrega un backlog estructurado.

**DiseÃ±ado para:**

- **Escalabilidad:** Procesar altos volÃºmenes mediante una arquitectura de pipeline desacoplado.
- **Gobernanza:** Flujo de datos estricto basado en contratos inmutables.
- **Extensibilidad:** Agregar nuevas fuentes o modelos sin refactorizar el nÃºcleo.

---

## ðŸ— Arquitectura y DiseÃ±o

Skema implementa una arquitectura de **Pipeline CanÃ³nico Orientado a Eventos**.
Abandonamos el monolito tradicional en favor de un diseÃ±o de **Pipes and Filters** unidireccional.

### Principios Fundamentales

1.  **Contracts First:** Nada se mueve entre mÃ³dulos sin un esquema de datos (DTO) definido.
2.  **API como Gateway:** La API es un adaptador "tonto". Solo valida, encola y responde. No contiene lÃ³gica de ML.
3.  **Observabilidad Sidecar:** Las mÃ©tricas y logs envuelven la lÃ³gica sin contaminarla.

### Flujo CanÃ³nico End-to-End

```mermaid
graph LR
    Input[ Fuentes: Jira/Slack ] --> API[ API Gateway]
    API -->|RawDocument| PIPE[ Pipeline Orchestrator ]

    subgraph "Internal Domain Logic"
    PIPE --> ING[Ingestion Adapter]
    ING --> PRE[Preprocessing]
    PRE -->|FeatureSet| INF[Inference Engine]
    end

    INF -->|PredictionResult| DB[( Storage )]
```

1.  **Ingestion:** Normaliza inputs externos al contrato `RawDocument`.
2.  **Preprocessing:** Limpieza determinista y extracciÃ³n de features (`FeatureSet`).
3.  **Inference:** OrÃ¡culo stateless. Aplica el modelo activo y emite `PredictionResult`.
4.  **Storage:** Persistencia de traza completa y notificaciÃ³n.

---

## ðŸ›  Estructura del Proyecto

Seguimos el patrÃ³n de "Screaming Architecture". La estructura comunica la intenciÃ³n del sistema.

```text
skema/
â”œâ”€â”€ contracts/          # La Ley. Schemas Pydantic compartidos e inmutables.
â”œâ”€â”€ cmd/                # Puntos de entrada (API Server, Worker, CLI).
â”œâ”€â”€ internal/           # LÃ³gica de Negocio Privada.
â”‚   â”œâ”€â”€ ingestion/      # Adaptadores de fuentes.
â”‚   â”œâ”€â”€ preprocessing/  # Funciones puras de limpieza.
â”‚   â”œâ”€â”€ inference/      # Estrategias de modelos ML.
â”‚   â””â”€â”€ pipeline/       # Orquestador del flujo.
â”œâ”€â”€ pkg/                # Utilidades compartidas (Logger, Metrics).
â””â”€â”€ deploy/             # Docker Compose e Infraestructura.
```

---

## ðŸš€ GuÃ­a RÃ¡pida

### Prerrequisitos

- Docker & Docker Compose
- Python 3.11+

### EjecuciÃ³n Local

Levanta todo el stack (API + Redis + Worker Mock):

```bash
docker-compose up -d --build
```

### Ingestar un requerimiento de prueba

```bash
curl -X POST http://localhost:8000/api/v1/ingest \
  -H "Content-Type: application/json" \
  -d '{"source": "ticket", "content": "El login falla con 500", "metadata": {}}'
```

---

## ðŸ—º Roadmap TÃ©cnico

- [ ] **v0.1:** DefiniciÃ³n de Arquitectura y Contratos Base (Actual).
- [ ] **v0.2:** ImplementaciÃ³n del Skeleton (FastAPI + Orquestador SÃ­ncrono).
- [ ] **v0.3:** IntegraciÃ³n de "Dummy Model" y Logging Estructurado.
- [ ] **v1.0:** Soporte asÃ­ncrono completo (Colas) y Persistencia DB.
